webpackJsonp([0xea08cedbaae5],{616:function(e,n){e.exports={pathContext:{docHtml:'<h1>Argo Workflow Templates by Example</h1>\n<h2>Welcome!</h2>\n<p>Argo is an open source project that provides container-native workflows for Kubernetes. Each step in an Argo workflow is defined as a container.</p>\n<p>Argo is implemented as a Kubernetes CRD (Custom Resource Definition). As a result, Argo workflows can be managed using <code>kubectl</code> and natively integrates with other Kubernetes services such as volumes, secrets, and RBAC. The new Argo software is light-weight and installs in under a minute, and provides complete workflow features including parameter substitution, artifacts, fixtures, loops and recursive workflows.</p>\n<p>Many of the Argo examples used in this walkthrough are available at <a href="https://github.com/argoproj/argo/tree/master/examples">https://github.com/argoproj/argo/tree/master/examples</a>. If you like this project, please give us a star!</p>\n<p>For a complete description of the Argo workflow spec, please refer to <a href="https://github.com/argoproj/argo/blob/master/pkg/apis/workflow/v1alpha1/types.go">https://github.com/argoproj/argo/blob/master/pkg/apis/workflow/v1alpha1/types.go</a></p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href="#argo-cli">Argo CLI</a></li>\n<li><a href="#hello-world">Hello World!</a></li>\n<li><a href="#parameters">Parameters</a></li>\n<li><a href="#steps">Steps</a></li>\n<li><a href="#dag">DAG</a></li>\n<li><a href="#artifacts">Artifacts</a></li>\n<li><a href="#the-structure-of-workflow-specs">The Structure of Workflow Specs</a></li>\n<li><a href="#secrets">Secrets</a></li>\n<li><a href="#scripts--results">Scripts &#x26; Results</a></li>\n<li><a href="#output-parameters">Output Parameters</a></li>\n<li><a href="#loops">Loops</a></li>\n<li><a href="#conditionals">Conditionals</a></li>\n<li><a href="#recursion">Recursion</a></li>\n<li><a href="#exit-handlers">Exit handlers</a></li>\n<li><a href="#timeouts">Timeouts</a></li>\n<li><a href="#volumes">Volumes</a></li>\n<li><a href="#daemon-containers">Daemon Containers</a></li>\n<li><a href="#sidecars">Sidecars</a></li>\n<li><a href="#hardwired-artifacts">Hardwired Artifacts</a></li>\n<li><a href="#kubernetes-resources">Kubernetes Resources</a></li>\n<li><a href="#docker-in-docker-aka-dind-using-sidecars">Docker-in-Docker Using Sidecars</a></li>\n<li><a href="#continuous-integration-example">Continuous Integration Example</a></li>\n</ul>\n<h2>Argo CLI</h2>\n<p>In case you want to follow along with this walkthrough, here\'s a quick overview of the most useful argo command line interface (CLI) commands.</p>\n<p><a href="https://github.com/argoproj/argo/blob/master/demo.md">Install Argo here</a></p>\n<pre><code class="language-sh">argo submit hello-world.yaml    # submit a workflow spec to Kubernetes\nargo list                       # list current workflows\nargo get hello-world-xxx        # get info about a specific workflow\nargo logs -w hello-world-xxx    # get logs from all steps in a workflow\nargo logs hello-world-xxx-yyy   # get logs from a specific step in a workflow\nargo delete hello-world-xxx     # delete workflow\n</code></pre>\n<p>You can also run workflow specs directly using <code>kubectl</code> but the Argo CLI provides syntax checking, nicer output, and requires less typing.</p>\n<pre><code class="language-sh">kubectl create -f hello-world.yaml\nkubectl get wf\nkubectl get wf hello-world-xxx\nkubectl get po --selector=workflows.argoproj.io/workflow=hello-world-xxx --show-all  # similar to argo\nkubectl logs hello-world-xxx-yyy -c main\nkubectl delete wf hello-world-xxx\n</code></pre>\n<h2>Hello World!</h2>\n<p>Let\'s start by creating a very simple workflow template to echo "hello world" using the docker/whalesay container image from DockerHub.</p>\n\n  <a class="gatsby-resp-image-link" href="/static/whalesay-aff8b868aac2a657e6626f22455396aa-1f4df.png" style="display: block" target="_blank" rel="noopener">\n  \n  <span class="gatsby-resp-image-wrapper" style="position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;">\n    <span class="gatsby-resp-image-background-image" style="padding-bottom: 11.190476190476192%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsSAAALEgHS3X78AAAAdUlEQVQI132MzQqCUBhEffQepH2P4CoICkGoVSC2aSHZpgT1/nv97lFcRwOzOANnsvzt2D8Nr9HTqkCrIy4KKSVEfnTdpymitNlYG4txnsvtwamsyA6NZVcpmm5ksAEvbNK/+BD4fDvmWej7YT23nK81x+LOAlpjmKfKCmu6AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;">\n      <img class="gatsby-resp-image-image" style="width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;" alt="whalesay" title="" src="/static/whalesay-aff8b868aac2a657e6626f22455396aa-fb8a0.png" srcset="/static/whalesay-aff8b868aac2a657e6626f22455396aa-1a291.png 148w,\n/static/whalesay-aff8b868aac2a657e6626f22455396aa-2bc4a.png 295w,\n/static/whalesay-aff8b868aac2a657e6626f22455396aa-fb8a0.png 590w,\n/static/whalesay-aff8b868aac2a657e6626f22455396aa-1f4df.png 840w" sizes="(max-width: 590px) 100vw, 590px">\n    </span>\n  </span>\n  \n  </a>\n    \n<p>You can run this directly from your shell with a simpleÂ docker command:</p>\n<pre><code>bash% docker run docker/whalesay cowsay "hello world"\n _____________\n&#x3C; hello world >\n -------------\n    \\\n     \\\n      \\\n                    ##        .\n              ## ## ##       ==\n           ## ## ## ##      ===\n       /""""""""""""""""___/ ===\n  ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~\n       \\______ o          __/\n        \\    \\        __/\n          \\____\\______/\n\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre>\n<p>Below, we run the same container on a Kubernetes cluster using an Argo workflow template.\nBe sure to read the comments as they provide useful explanations.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow                  # new type of k8s spec\nmetadata:\n  generateName: hello-world-    # name of the workflow spec\nspec:\n  entrypoint: whalesay          # invoke the whalesay template\n  templates:\n  - name: whalesay              # name of the template\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: ["hello world"]\n      resources:                # limit the resources\n        limits:\n          memory: 32Mi\n          cpu: 100m\n</code></pre>\n<p>Argo adds a new <code>kind</code> of Kubernetes spec called a <code>Workflow</code>. The above spec contains a single <code>template</code> called <code>whalesay</code> which runs the <code>docker/whalesay</code> container and invokes <code>cowsay "hello world"</code>. The <code>whalesay</code> template is the <code>entrypoint</code> for the spec. The entrypoint specifies the initial template that should be invoked when the workflow spec is executed by Kubernetes. Being able to specify the entrypoint is more useful when there is more than one template defined in the Kubernetes workflow spec. :-)</p>\n<h2>Parameters</h2>\n<p>Let\'s look at a slightly more complex workflow spec with parameters.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-parameters-\nspec:\n  # invoke the whalesay template with\n  # "hello world" as the argument\n  # to the message parameter\n  entrypoint: whalesay\n  arguments:\n    parameters:\n    - name: message\n      value: hello world\n\n  templates:\n  - name: whalesay\n    inputs:\n      parameters:\n      - name: message       # parameter declaration\n    container:\n      # run cowsay with that message input parameter as args\n      image: docker/whalesay\n      command: [cowsay]\n      args: ["{{inputs.parameters.message}}"]\n</code></pre>\n<p>This time, the <code>whalesay</code> template takes an input parameter named <code>message</code> that is passed as the <code>args</code> to the <code>cowsay</code> command. In order to reference parameters (e.g., <code>"{{inputs.parameters.message}}"</code>), the parameters must be enclosed in double quotes to escape the curly braces in YAML.</p>\n<p>The argo CLI provides a convenient way to override parameters used to invoke the entrypoint. For example, the following command would bind the <code>message</code> parameter to "goodbye world" instead of the default "hello world".</p>\n<pre><code class="language-sh">argo submit arguments-parameters.yaml -p message="goodbye world"\n</code></pre>\n<p>In case of multiple parameters that can be overriten, the argo CLI provides a command to load parameters files in YAML or JSON format. Here is an example of that kind of parameter file:</p>\n<pre><code class="language-yaml">message: goodbye world\n</code></pre>\n<p>To run use following command:</p>\n<pre><code class="language-sh">argo submit arguments-parameters.yaml --parameter-file params.yaml\n</code></pre>\n<p>Command-line parameters can also be used to override the default entrypoint and invoke any template in the workflow spec. For example, if you add a new version of the <code>whalesay</code> template called <code>whalesay-caps</code> but you don\'t want to change the default entrypoint, you can invoke this from the command line as follows:</p>\n<pre><code class="language-sh">argo submit arguments-parameters.yaml --entrypoint whalesay-caps\n</code></pre>\n<p>By using a combination of the <code>--entrypoint</code> and <code>-p</code> parameters, you can call any template in the workflow spec with any parameter that you like.</p>\n<p>The values set in the <code>spec.arguments.parameters</code> are globally scoped and can be accessed via <code>{{workflow.parameters.parameter_name}}</code>. This can be useful to pass information to multiple steps in a workflow. For example, if you wanted to run your workflows with different logging levels that are set in the environment of each container, you could have a YAML file similar to this one:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: global-parameters-\nspec:\n  entrypoint: A\n  arguments:\n    parameters:\n    - name: log-level\n      value: INFO\n\n  templates:\n  - name: A\n    container:\n      image: containerA\n      env:\n      - name: LOG_LEVEL\n        value: "{{workflow.parameters.log-level}}"\n      command: [runA]\n  - name: B\n    container:\n      image: containerB\n      env:\n      - name: LOG_LEVEL\n        value: "{{workflow.parameters.log-level}}"\n      command: [runB]\n</code></pre>\n<p>In this workflow, both steps <code>A</code> and <code>B</code> would have the same log-level set to <code>INFO</code> and can easily be changed between workflow submissions using the <code>-p</code> flag.</p>\n<h2>Steps</h2>\n<p>In this example, we\'ll see how to create multi-step workflows, how to define more than one template in a workflow spec, and how to create nested workflows. Be sure to read the comments as they provide useful explanations.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: steps-\nspec:\n  entrypoint: hello-hello-hello\n\n  # This spec contains two templates: hello-hello-hello and whalesay\n  templates:\n  - name: hello-hello-hello\n    # Instead of just running a container\n    # This template has a sequence of steps\n    steps:\n    - - name: hello1            # hello1 is run before the following steps\n        template: whalesay\n        arguments:\n          parameters:\n          - name: message\n            value: "hello1"\n    - - name: hello2a           # double dash => run after previous step\n        template: whalesay\n        arguments:\n          parameters:\n          - name: message\n            value: "hello2a"\n      - name: hello2b           # single dash => run in parallel with previous step\n        template: whalesay\n        arguments:\n          parameters:\n          - name: message\n            value: "hello2b"\n\n  # This is the same template as from the previous example\n  - name: whalesay\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: ["{{inputs.parameters.message}}"]\n</code></pre>\n<p>The above workflow spec prints three different flavors of "hello". The <code>hello-hello-hello</code> template consists of three <code>steps</code>. The first step named <code>hello1</code> will be run in sequence whereas the next two steps named <code>hello2a</code> and <code>hello2b</code> will be run in parallel with each other. Using the argo CLI command, we can graphically display the execution history of this workflow spec, which shows that the steps named <code>hello2a</code> and <code>hello2b</code> ran in parallel with each other.</p>\n<pre><code>STEP                                     PODNAME\n â arguments-parameters-rbm92\n â---â hello1                   steps-rbm92-2023062412\n â-Â·-â hello2a                  steps-rbm92-685171357\n   â-â hello2b                  steps-rbm92-634838500\n</code></pre>\n<h2>DAG</h2>\n<p>As an alternative to specifying sequences of steps, you can define the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each task. This can be simpler to maintain for complex workflows and allows for maximum parallelism when running tasks.</p>\n<p>In the following workflow, step <code>A</code> runs first, as it has no dependencies. Once <code>A</code> has finished, steps <code>B</code> and <code>C</code> run in parallel. Finally, once <code>B</code> and <code>C</code> have completed, step <code>D</code> can run.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-diamond-\nspec:\n  entrypoint: diamond\n  templates:\n  - name: echo\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: alpine:3.7\n      command: [echo, "{{inputs.parameters.message}}"]\n  - name: diamond\n    dag:\n      tasks:\n      - name: A\n        template: echo\n        arguments:\n          parameters: [{name: message, value: A}]\n      - name: B\n        dependencies: [A]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: B}]\n      - name: C\n        dependencies: [A]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: C}]\n      - name: D\n        dependencies: [B, C]\n        template: echo\n        arguments:\n          parameters: [{name: message, value: D}]\n</code></pre>\n<p>The dependency graph may have <a href="./dag-multiroot.yaml">multiple roots</a>. The templates called from a DAG or steps template can themselves be DAG or steps templates. This can allow for complex workflows to be split into manageable pieces.</p>\n<h2>Artifacts</h2>\n<p><strong>Note:</strong>\nYou will need to configure an artifact repository to run this example.\n<a href="https://github.com/argoproj/argo/blob/master/ARTIFACT_REPO.md">Configuring an artifact repository here</a>.</p>\n<p>When running workflows, it is very common to have steps that generate or consume artifacts. Often, the output artifacts of one step may be used as input artifacts to a subsequent step.</p>\n<p>The below workflow spec consists of two steps that run in sequence. The first step named <code>generate-artifact</code> will generate an artifact using the <code>whalesay</code> template that will be consumed by the second step named <code>print-message</code> that then consumes the generated artifact.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-passing-\nspec:\n  entrypoint: artifact-example\n  templates:\n  - name: artifact-example\n    steps:\n    - - name: generate-artifact\n        template: whalesay\n    - - name: consume-artifact\n        template: print-message\n        arguments:\n          artifacts:\n          # bind message to the hello-art artifact\n          # generated by the generate-artifact step\n          - name: message\n            from: "{{steps.generate-artifact.outputs.artifacts.hello-art}}"\n\n  - name: whalesay\n    container:\n      image: docker/whalesay:latest\n      command: [sh, -c]\n      args: ["cowsay hello world | tee /tmp/hello_world.txt"]\n    outputs:\n      artifacts:\n      # generate hello-art artifact from /tmp/hello_world.txt\n      # artifacts can be directories as well as files\n      - name: hello-art\n        path: /tmp/hello_world.txt\n\n  - name: print-message\n    inputs:\n      artifacts:\n      # unpack the message input artifact\n      # and put it at /tmp/message\n      - name: message\n        path: /tmp/message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["cat /tmp/message"]\n</code></pre>\n<p>The <code>whalesay</code> template uses the <code>cowsay</code> command to generate a file named <code>/tmp/hello-world.txt</code>. It then <code>outputs</code> this file as an artifact named <code>hello-art</code>. In general, the artifact\'s <code>path</code> may be a directory rather than just a file. The <code>print-message</code> template takes an input artifact named <code>message</code>, unpacks it at the <code>path</code> named <code>/tmp/message</code> and then prints the contents of <code>/tmp/message</code> using the <code>cat</code> command.\nThe <code>artifact-example</code> template passes the <code>hello-art</code> artifact generated as an output of the <code>generate-artifact</code> step as the <code>message</code> input artifact to the <code>print-message</code> step. DAG templates use the tasks prefix to refer to another task, for example <code>{{tasks.generate-artifact.outputs.artifacts.hello-art}}</code>.</p>\n<h2>The Structure of Workflow Specs</h2>\n<p>We now know enough about the basic components of a workflow spec to review its basic structure:</p>\n<ul>\n<li>\n<p>Kubernetes header including metadata</p>\n</li>\n<li>\n<p>Spec body</p>\n<ul>\n<li>Entrypoint invocation with optionally arguments</li>\n<li>List of template definitions</li>\n</ul>\n</li>\n<li>\n<p>For each template definition</p>\n<ul>\n<li>Name of the template</li>\n<li>Optionally a list of inputs</li>\n<li>Optionally a list of outputs</li>\n<li>Container invocation (leaf template) or a list of steps</li>\n<li>For each step, a template invocation</li>\n</ul>\n</li>\n</ul>\n<p>To summarize, workflow specs are composed of a set of Argo templates where each template consists of an optional input section, an optional output section and either a container invocation or a list of steps where each step invokes another template.</p>\n<p>Note that the controller section of the workflow spec will accept the same options as the controller section of a pod spec, including but not limited to environment variables, secrets, and volume mounts. Similarly, for volume claims and volumes.</p>\n<h2>Secrets</h2>\n<p>Argo supports the same secrets syntax and mechanisms as Kubernetes Pod specs, which allows access to secrets as environment variables or volume mounts. See the (Kubernetes documentation)[https://kubernetes.io/docs/concepts/configuration/secret/] for more information.</p>\n<pre><code class="language-yaml"># To run this example, first create the secret by running:\n# kubectl create secret generic my-secret --from-literal=mypassword=S00perS3cretPa55word\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: whalesay\n  # To access secrets as files, add a volume entry in spec.volumes[] and\n  # then in the container template spec, add a mount using volumeMounts.\n  volumes:\n  - name: my-secret-vol\n    secret:\n      secretName: my-secret     # name of an existing k8s secret\n  templates:\n  - name: whalesay\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: [\'\n        echo "secret from env: $MYSECRETPASSWORD";\n        echo "secret from file: `cat /secret/mountpath/mypassword`"\n      \']\n      # To access secrets as environment variables, use the k8s valueFrom and\n      # secretKeyRef constructs.\n      env:\n      - name: MYSECRETPASSWORD  # name of env var\n        valueFrom:\n          secretKeyRef:\n            name: my-secret     # name of an existing k8s secret\n            key: mypassword     # \'key\' subcomponent of the secret\n      volumeMounts:\n      - name: my-secret-vol     # mount file containing secret at /secret/mountpath\n        mountPath: "/secret/mountpath"\n</code></pre>\n<h2>Scripts &#x26; Results</h2>\n<p>Often, we just want a template that executes a script specified as a here-script (also known as a <code>here document</code>) in the workflow spec. This example shows how to do that:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: scripts-bash-\nspec:\n  entrypoint: bash-script-example\n  templates:\n  - name: bash-script-example\n    steps:\n    - - name: generate\n        template: gen-random-int-bash\n    - - name: print\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: "{{steps.generate.outputs.result}}"  # The result of the here-script\n\n  - name: gen-random-int-bash\n    script:\n      image: debian:9.4\n      command: [bash]\n      source: |                                         # Contents of the here-script\n        cat /dev/urandom | od -N2 -An -i | awk -v f=1 -v r=100 \'{printf "%i\\n", f + r * $1 / 65536}\'\n\n  - name: gen-random-int-python\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        i = random.randint(1, 100)\n        print(i)\n\n  - name: gen-random-int-javascript\n    script:\n      image: node:9.1-alpine\n      command: [node]\n      source: |\n        var rand = Math.floor(Math.random() * 100);\n        console.log(rand);\n\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo result was: {{inputs.parameters.message}}"]\n</code></pre>\n<p>The <code>script</code> keyword allows the specification of the script body using the <code>source</code> tag. This creates a temporary file containing the script body and then passes the name of the temporary file as the final parameter to <code>command</code>, which should be an interpreter that executes the script body.</p>\n<p>The use of the <code>script</code> feature also assigns the standard output of running the script to a special output parameter named <code>result</code>. This allows you to use the result of running the script itself in the rest of the workflow spec. In this example, the result is simply echoed by the print-message template.</p>\n<h2>Output Parameters</h2>\n<p>Output parameters provide a general mechanism to use the result of a step as a parameter rather than as an artifact. This allows you to use the result from any type of step, not just a <code>script</code>, for conditional tests, loops, and arguments. Output parameters work similarly to <code>script result</code> except that the value of the output parameter is set to the contents of a generated file rather than the contents of <code>stdout</code>.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: output-parameter-\nspec:\n  entrypoint: output-parameter\n  templates:\n  - name: output-parameter\n    steps:\n    - - name: generate-parameter\n        template: whalesay\n    - - name: consume-parameter\n        template: print-message\n        arguments:\n          parameters:\n          # Pass the hello-param output from the generate-parameter step as the message input to print-message\n          - name: message\n            value: "{{steps.generate-parameter.outputs.parameters.hello-param}}"\n\n  - name: whalesay\n    container:\n      image: docker/whalesay:latest\n      command: [sh, -c]\n      args: ["echo -n hello world > /tmp/hello_world.txt"]  # generate the content of hello_world.txt\n    outputs:\n      parameters:\n      - name: hello-param       # name of output parameter\n        valueFrom:\n          path: /tmp/hello_world.txt    # set the value of hello-param to the contents of this hello-world.txt\n\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: docker/whalesay:latest\n      command: [cowsay]\n      args: ["{{inputs.parameters.message}}"]\n</code></pre>\n<p>DAG templates use the tasks prefix to refer to another task, for example <code>{{tasks.generate-parameter.outputs.parameters.hello-param}}</code>.</p>\n<h2>Loops</h2>\n<p>When writing workflows, it is often very useful to be able to iterate over a set of inputs as shown in this example:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-\nspec:\n  entrypoint: loop-example\n  templates:\n  - name: loop-example\n    steps:\n    - - name: print-message\n        template: whalesay\n        arguments:\n          parameters:\n          - name: message\n            value: "{{item}}"\n        withItems:              # invoke whalesay once for each item in parallel\n        - hello world           # item 1\n        - goodbye world         # item 2\n\n  - name: whalesay\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: docker/whalesay:latest\n      command: [cowsay]\n      args: ["{{inputs.parameters.message}}"]\n</code></pre>\n<p>We can also iterate over sets of items:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-maps-\nspec:\n  entrypoint: loop-map-example\n  templates:\n  - name: loop-map-example\n    steps:\n    - - name: test-linux\n        template: cat-os-release\n        arguments:\n          parameters:\n          - name: image\n            value: "{{item.image}}"\n          - name: tag\n            value: "{{item.tag}}"\n        withItems:\n        - { image: \'debian\', tag: \'9.1\' }       #item set 1\n        - { image: \'debian\', tag: \'8.9\' }       #item set 2\n        - { image: \'alpine\', tag: \'3.6\' }       #item set 3\n        - { image: \'ubuntu\', tag: \'17.10\' }     #item set 4\n\n  - name: cat-os-release\n    inputs:\n      parameters:\n      - name: image\n      - name: tag\n    container:\n      image: "{{inputs.parameters.image}}:{{inputs.parameters.tag}}"\n      command: [cat]\n      args: [/etc/os-release]\n</code></pre>\n<p>We can pass lists of items as parameters:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-param-arg-\nspec:\n  entrypoint: loop-param-arg-example\n  arguments:\n    parameters:\n    - name: os-list                                     # a list of items\n      value: |\n        [\n          { "image": "debian", "tag": "9.1" },\n          { "image": "debian", "tag": "8.9" },\n          { "image": "alpine", "tag": "3.6" },\n          { "image": "ubuntu", "tag": "17.10" }\n        ]\n\n  templates:\n  - name: loop-param-arg-example\n    inputs:\n      parameters:\n      - name: os-list\n    steps:\n    - - name: test-linux\n        template: cat-os-release\n        arguments:\n          parameters:\n          - name: image\n            value: "{{item.image}}"\n          - name: tag\n            value: "{{item.tag}}"\n        withParam: "{{inputs.parameters.os-list}}"      # parameter specifies the list to iterate over\n\n  # This template is the same as in the previous example\n  - name: cat-os-release\n    inputs:\n      parameters:\n      - name: image\n      - name: tag\n    container:\n      image: "{{inputs.parameters.image}}:{{inputs.parameters.tag}}"\n      command: [cat]\n      args: [/etc/os-release]\n</code></pre>\n<p>We can even dynamically generate the list of items to iterate over!</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: loops-param-result-\nspec:\n  entrypoint: loop-param-result-example\n  templates:\n  - name: loop-param-result-example\n    steps:\n    - - name: generate\n        template: gen-number-list\n    # Iterate over the list of numbers generated by the generate step above\n    - - name: sleep\n        template: sleep-n-sec\n        arguments:\n          parameters:\n          - name: seconds\n            value: "{{item}}"\n        withParam: "{{steps.generate.outputs.result}}"\n\n  # Generate a list of numbers in JSON format\n  - name: gen-number-list\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import json\n        import sys\n        json.dump([i for i in range(20, 31)], sys.stdout)\n\n  - name: sleep-n-sec\n    inputs:\n      parameters:\n      - name: seconds\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo sleeping for {{inputs.parameters.seconds}} seconds; sleep {{inputs.parameters.seconds}}; echo done"]\n</code></pre>\n<h2>Conditionals</h2>\n<p>We also support conditional execution as shown in this example:</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: coinflip-\nspec:\n  entrypoint: coinflip\n  templates:\n  - name: coinflip\n    steps:\n    # flip a coin\n    - - name: flip-coin\n        template: flip-coin\n    # evaluate the result in parallel\n    - - name: heads\n        template: heads                 # call heads template if "heads"\n        when: "{{steps.flip-coin.outputs.result}} == heads"\n      - name: tails\n        template: tails                 # call tails template if "tails"\n        when: "{{steps.flip-coin.outputs.result}} == tails"\n\n  # Return heads or tails based on a random number\n  - name: flip-coin\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        result = "heads" if random.randint(0,1) == 0 else "tails"\n        print(result)\n\n  - name: heads\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: ["echo \\"it was heads\\""]\n\n  - name: tails\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: ["echo \\"it was tails\\""]\n</code></pre>\n<h2>Recursion</h2>\n<p>Templates can recursively invoke each other! In this variation of the above coin-flip template, we continue to flip coins until it comes up heads.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: coinflip-recursive-\nspec:\n  entrypoint: coinflip\n  templates:\n  - name: coinflip\n    steps:\n    # flip a coin\n    - - name: flip-coin\n        template: flip-coin\n    # evaluate the result in parallel\n    - - name: heads\n        template: heads                 # call heads template if "heads"\n        when: "{{steps.flip-coin.outputs.result}} == heads"\n      - name: tails                     # keep flipping coins if "tails"\n        template: coinflip\n        when: "{{steps.flip-coin.outputs.result}} == tails"\n\n  - name: flip-coin\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        result = "heads" if random.randint(0,1) == 0 else "tails"\n        print(result)\n\n  - name: heads\n    container:\n      image: alpine:3.6\n      command: [sh, -c]\n      args: ["echo \\"it was heads\\""]\n</code></pre>\n<p>Here\'s the result of a couple of runs of coinflip for comparison.</p>\n<pre><code>argo get coinflip-recursive-tzcb5\n\nSTEP                         PODNAME                              MESSAGE\n â coinflip-recursive-vhph5\n â---â flip-coin             coinflip-recursive-vhph5-2123890397\n â-Â·-â heads                 coinflip-recursive-vhph5-128690560\n   â-â tails\n\nSTEP                          PODNAME                              MESSAGE\n â coinflip-recursive-tzcb5\n â---â flip-coin              coinflip-recursive-tzcb5-322836820\n â-Â·-â heads\n   â-â tails\n     â---â flip-coin          coinflip-recursive-tzcb5-1863890320\n     â-Â·-â heads\n       â-â tails\n         â---â flip-coin      coinflip-recursive-tzcb5-1768147140\n         â-Â·-â heads\n           â-â tails\n             â---â flip-coin  coinflip-recursive-tzcb5-4080411136\n             â-Â·-â heads      coinflip-recursive-tzcb5-4080323273\n               â-â tails\n</code></pre>\n<p>In the first run, the coin immediately comes up heads and we stop. In the second run, the coin comes up tail three times before it finally comes up heads and we stop.</p>\n<h2>Exit handlers</h2>\n<p>An exit handler is a template that <em>always</em> executes, irrespective of success or failure, at the end of the workflow.</p>\n<p>Some common use cases of exit handlers are:</p>\n<ul>\n<li>cleaning up after a workflow runs</li>\n<li>sending notifications of workflow status (e.g., e-mail/Slack)</li>\n<li>posting the pass/fail status to a webhook result (e.g. GitHub build result)</li>\n<li>resubmitting or submitting another workflow</li>\n</ul>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: exit-handlers-\nspec:\n  entrypoint: intentional-fail\n  onExit: exit-handler                  # invoke exit-hander template at end of the workflow\n  templates:\n  # primary workflow template\n  - name: intentional-fail\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo intentional failure; exit 1"]\n\n  # Exit handler templates\n  # After the completion of the entrypoint template, the status of the\n  # workflow is made available in the global variable {{workflow.status}}.\n  # {{workflow.status}} will be one of: Succeeded, Failed, Error\n  - name: exit-handler\n    steps:\n    - - name: notify\n        template: send-email\n      - name: celebrate\n        template: celebrate\n        when: "{{workflow.status}} == Succeeded"\n      - name: cry\n        template: cry\n        when: "{{workflow.status}} != Succeeded"\n  - name: send-email\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo send e-mail: {{workflow.name}} {{workflow.status}}"]\n  - name: celebrate\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo hooray!"]\n  - name: cry\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo boohoo!"]\n</code></pre>\n<h2>Timeouts</h2>\n<p>To limit the elapsed time for a workflow, you can set the variable <code>activeDeadlineSeconds</code>.</p>\n<pre><code class="language-yaml"># To enforce a timeout for a container template, specify a value for activeDeadlineSeconds.\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  entrypoint: sleep\n  templates:\n  - name: sleep\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo sleeping for 1m; sleep 60; echo done"]\n    activeDeadlineSeconds: 10           # terminate container template after 10 seconds\n</code></pre>\n<h2>Volumes</h2>\n<p>The following example dynamically creates a volume and then uses the volume in a two step workflow.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: volumes-pvc-\nspec:\n  entrypoint: volumes-pvc-example\n  volumeClaimTemplates:                 # define volume, same syntax as k8s Pod spec\n  - metadata:\n      name: workdir                     # name of volume claim\n    spec:\n      accessModes: [ "ReadWriteOnce" ]\n      resources:\n        requests:\n          storage: 1Gi                  # Gi => 1024 * 1024 * 1024\n\n  templates:\n  - name: volumes-pvc-example\n    steps:\n    - - name: generate\n        template: whalesay\n    - - name: print\n        template: print-message\n\n  - name: whalesay\n    container:\n      image: docker/whalesay:latest\n      command: [sh, -c]\n      args: ["echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt"]\n      # Mount workdir volume at /mnt/vol before invoking docker/whalesay\n      volumeMounts:                     # same syntax as k8s Pod spec\n      - name: workdir\n        mountPath: /mnt/vol\n\n  - name: print-message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt"]\n      # Mount workdir volume at /mnt/vol before invoking docker/whalesay\n      volumeMounts:                     # same syntax as k8s Pod spec\n      - name: workdir\n        mountPath: /mnt/vol\n</code></pre>\n<p>Volumes are a very useful way to move large amounts of data from one step in a workflow to another. Depending on the system, some volumes may be accessible concurrently from multiple steps.</p>\n<p>In some cases, you want to access an already existing volume rather than creating/destroying one dynamically.</p>\n<pre><code class="language-yaml"># Define Kubernetes PVC\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: my-existing-volume\nspec:\n  accessModes: [ "ReadWriteOnce" ]\n  resources:\n    requests:\n      storage: 1Gi\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: volumes-existing-\nspec:\n  entrypoint: volumes-existing-example\n  volumes:\n  # Pass my-existing-volume as an argument to the volumes-existing-example template\n  # Same syntax as k8s Pod spec\n  - name: workdir\n    persistentVolumeClaim:\n      claimName: my-existing-volume\n\n  templates:\n  - name: volumes-existing-example\n    steps:\n    - - name: generate\n        template: whalesay\n    - - name: print\n        template: print-message\n\n  - name: whalesay\n    container:\n      image: docker/whalesay:latest\n      command: [sh, -c]\n      args: ["echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n\n  - name: print-message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: ["echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt"]\n      volumeMounts:\n      - name: workdir\n        mountPath: /mnt/vol\n</code></pre>\n<h2>Daemon Containers</h2>\n<p>Argo workflows can start containers that run in the background (also known as <code>daemon containers</code>) while the workflow itself continues execution. Note that the daemons will be <em>automatically destroyed</em> when the workflow exits the template scope in which the daemon was invoked. Deamon containers are useful for starting up services to be tested or to be used in testing (e.g., fixtures). We also find it very useful when running large simulations to spin up a database as a daemon for collecting and organizing the results. The big advantage of daemons compared with sidecars is that their existence can persist across multiple steps or even the entire workflow.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: daemon-step-\nspec:\n  entrypoint: daemon-example\n  templates:\n  - name: daemon-example\n    steps:\n    - - name: influx\n        template: influxdb              # start an influxdb as a daemon (see the influxdb template spec below)\n\n    - - name: init-database             # initialize influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl -XPOST \'http://{{steps.influx.ip}}:8086/query\' --data-urlencode "q=CREATE DATABASE mydb"\n\n    - - name: producer-1                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: for i in $(seq 1 20); do curl -XPOST \'http://{{steps.influx.ip}}:8086/write?db=mydb\' -d "cpu,host=server01,region=uswest load=$i" ; sleep .5 ; done\n      - name: producer-2                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: for i in $(seq 1 20); do curl -XPOST \'http://{{steps.influx.ip}}:8086/write?db=mydb\' -d "cpu,host=server02,region=uswest load=$((RANDOM % 100))" ; sleep .5 ; done\n      - name: producer-3                # add entries to influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl -XPOST \'http://{{steps.influx.ip}}:8086/write?db=mydb\' -d \'cpu,host=server03,region=useast load=15.4\'\n\n    - - name: consumer                  # consume intries from influxdb\n        template: influxdb-client\n        arguments:\n          parameters:\n          - name: cmd\n            value: curl --silent -G http://{{steps.influx.ip}}:8086/query?pretty=true --data-urlencode "db=mydb" --data-urlencode "q=SELECT * FROM cpu"\n\n  - name: influxdb\n    daemon: true                        # start influxdb as a daemon\n    container:\n      image: influxdb:1.2\n      restartPolicy: Always             # restart container if it fails\n      readinessProbe:                   # wait for readinessProbe to succeed\n        httpGet:\n          path: /ping\n          port: 8086\n\n  - name: influxdb-client\n    inputs:\n      parameters:\n      - name: cmd\n    container:\n      image: appropriate/curl:latest\n      command: ["/bin/sh", "-c"]\n      args: ["{{inputs.parameters.cmd}}"]\n      resources:\n        requests:\n          memory: 32Mi\n          cpu: 100m\n</code></pre>\n<p>DAG templates use the tasks prefix to refer to another task, for example <code>{{tasks.influx.ip}}</code>.</p>\n<h2>Sidecars</h2>\n<p>A sidecar is another container that executes concurrently in the same pod as the main container and is useful in creating multi-container pods.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: sidecar-nginx-\nspec:\n  entrypoint: sidecar-nginx-example\n  templates:\n  - name: sidecar-nginx-example\n    container:\n      image: appropriate/curl\n      command: [sh, -c]\n      # Try to read from nginx web server until it comes up\n      args: ["until `curl -G \'http://127.0.0.1/\' >&#x26; /tmp/out`; do echo sleep &#x26;&#x26; sleep 1; done &#x26;&#x26; cat /tmp/out"]\n    # Create a simple nginx web server\n    sidecars:\n    - name: nginx\n      image: nginx:1.13\n</code></pre>\n<p>In the above example, we create a sidecar container that runs nginx as a simple web server. The order in which containers come up is random, so in this example the main container polls the nginx container until it is ready to service requests. This is a good design pattern when designing multi-container systems: always wait for any services you need to come up before running your main code.</p>\n<h2>Hardwired Artifacts</h2>\n<p>With Argo, you can use any container image that you like to generate any kind of artifact. In practice, however, we find certain types of artifacts are very common, so there is built-in support for git, http, and s3 artifacts.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hardwired-artifact-\nspec:\n  entrypoint: hardwired-artifact\n  templates:\n  - name: hardwired-artifact\n    inputs:\n      artifacts:\n      # Check out the master branch of the argo repo and place it at /src\n      # revision can be anything that git checkout accepts: branch, commit, tag, etc.\n      - name: argo-source\n        path: /src\n        git:\n          repo: https://github.com/argoproj/argo.git\n          revision: "master"\n      # Download kubectl 1.8.0 and place it at /bin/kubectl\n      - name: kubectl\n        path: /bin/kubectl\n        mode: 0755\n        http:\n          url: https://storage.googleapis.com/kubernetes-release/release/v1.8.0/bin/linux/amd64/kubectl\n      # Copy an s3 bucket and place it at /s3\n      - name: objects\n        path: /s3\n        s3:\n          endpoint: storage.googleapis.com\n          bucket: my-bucket-name\n          key: path/in/bucket\n          accessKeySecret:\n            name: my-s3-credentials\n            key: accessKey\n          secretKeySecret:\n            name: my-s3-credentials\n            key: secretKey\n    container:\n      image: debian\n      command: [sh, -c]\n      args: ["ls -l /src /bin/kubectl /s3"]\n</code></pre>\n<h2>Kubernetes Resources</h2>\n<p>In many cases, you will want to manage Kubernetes resources from Argo workflows. The resource template allows you to create, delete or updated any type of Kubernetes resource.</p>\n<pre><code class="language-yaml"># in a workflow. The resource template type accepts any k8s manifest\n# (including CRDs) and can perform any kubectl action against it (e.g. create,\n# apply, delete, patch).\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: k8s-jobs-\nspec:\n  entrypoint: pi-tmpl\n  templates:\n  - name: pi-tmpl\n    resource:                   # indicates that this is a resource template\n      action: create            # can be any kubectl action (e.g. create, delete, apply, patch)\n      # The successCondition and failureCondition are optional expressions.\n      # If failureCondition is true, the step is considered failed.\n      # If successCondition is true, the step is considered successful.\n      # They use kubernetes label selection syntax and can be applied against any field\n      # of the resource (not just labels). Multiple AND conditions can be represented by comma\n      # delimited expressions.\n      # For more details: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n      successCondition: status.succeeded > 0\n      failureCondition: status.failed > 3\n      manifest: |               #put your kubernetes spec here\n        apiVersion: batch/v1\n        kind: Job\n        metadata:\n          generateName: pi-job-\n        spec:\n          template:\n            metadata:\n              name: pi\n            spec:\n              containers:\n              - name: pi\n                image: perl\n                command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]\n              restartPolicy: Never\n          backoffLimit: 4\n</code></pre>\n<p>Resources created in this way are independent of the workflow. If you want the resource to be deleted when the workflow is deleted then you can use <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">Kubernetes garbage collection</a> with the workflow resource as an owner reference (<a href="./k8s-owner-reference.yaml">example</a>).</p>\n<h2>Docker-in-Docker Using Sidecars</h2>\n<p>An application of sidecars is to implement Docker-in-Docker (DinD). DinD is useful when you want to run Docker commands from inside a container. For example, you may want to build and push a container image from inside your build container. In the following example, we use the docker:dind container to run a Docker daemon in a sidecar and give the main container access to the daemon.</p>\n<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: sidecar-dind-\nspec:\n  entrypoint: dind-sidecar-example\n  templates:\n  - name: dind-sidecar-example\n    container:\n      image: docker:17.10\n      command: [sh, -c]\n      args: ["until docker ps; do sleep 3; done; docker run --rm debian:latest cat /etc/os-release"]\n      env:\n      - name: DOCKER_HOST               # the docker daemon can be access on the standard port on localhost\n        value: 127.0.0.1\n    sidecars:\n    - name: dind\n      image: docker:17.10-dind          # Docker already provides an image for running a Docker daemon\n      securityContext:\n        privileged: true                # the Docker daemon can only run in a privileged container\n      # mirrorVolumeMounts will mount the same volumes specified in the main container\n      # to the sidecar (including artifacts), at the same mountPaths. This enables\n      # dind daemon to (partially) see the same filesystem as the main container in\n      # order to use features such as docker volume binding.\n      mirrorVolumeMounts: true\n</code></pre>\n<h2>Continuous Integration Example</h2>\n<p>Continuous integration is a popular application for workflows. Currently, Argo does not provide event triggers for automatically kicking off your CI jobs, but we plan to do so in the near future. Until then, you can easily write a cron job that checks for new commits and kicks off the needed workflow, or use your existing Jenkins server to kick off the workflow.</p>\n<p>A good example of a CI workflow spec is provided at <a href="https://github.com/argoproj/argo/tree/master/examples/influxdb-ci.yaml">https://github.com/argoproj/argo/tree/master/examples/influxdb-ci.yaml</a>. Because it just uses the concepts that we\'ve already covered and is somewhat long, we don\'t go into details here.</p>',
docPath:"argo/examples/readme",proj:"argo"}}}});
//# sourceMappingURL=path---docs-argo-examples-readme-html-7fc0a056db30c68b6021.js.map