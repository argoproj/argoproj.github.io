webpackJsonp([0xe2bc9d88471e],{2468:function(e,o){e.exports={pathContext:{docHtml:'<h1>has-unicode</h1>\n<p>Try to guess if your terminal supports unicode</p>\n<pre><code class="language-javascript">var hasUnicode = require("has-unicode")\n\nif (hasUnicode()) {\n  // the terminal probably has unicode support\n}\n</code></pre>\n<pre><code class="language-javascript">var hasUnicode = require("has-unicode").tryHarder\nhasUnicode(function(unicodeSupported) {\n  if (unicodeSupported) {\n    // the terminal probably has unicode support\n  }\n})\n</code></pre>\n<h2>Detecting Unicode</h2>\n<p>What we actually detect is UTF-8 support, as that\'s what Node itself supports.\nIf you have a UTF-16 locale then you won\'t be detected as unicode capable.</p>\n<h3>Windows</h3>\n<p>Since at least Windows 7, <code>cmd</code> and <code>powershell</code> have been unicode capable,\nbut unfortunately even then it\'s not guaranteed. In many localizations it\nstill uses legacy code pages and there\'s no facility short of running\nprograms or linking C++ that will let us detect this. As such, we\nreport any Windows installation as NOT unicode capable, and recommend\nthat you encourage your users to override this via config.</p>\n<h3>Unix Like Operating Systems</h3>\n<p>We look at the environment variables <code>LC_ALL</code>, <code>LC_CTYPE</code>, and <code>LANG</code> in\nthat order.  For <code>LC_ALL</code> and <code>LANG</code>, it looks for <code>.UTF-8</code> in the value.\nFor <code>LC_CTYPE</code> it looks to see if the value is <code>UTF-8</code>.  This is sufficient\nfor most POSIX systems.  While locale data can be put in <code>/etc/locale.conf</code>\nas well, AFAIK it\'s always copied into the environment.</p>',docPath:"argo-ci/node_modules/fsevents/node_modules/has-unicode/readme",proj:"argo-ci"}}}});
//# sourceMappingURL=path---docs-argo-ci-node-modules-fsevents-node-modules-has-unicode-03367d19c1d8824007b0.js.map